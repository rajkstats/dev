{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Running Spark with Notebooks\n",
    "\n",
    "- toc: false\n",
    "- badges: true\n",
    "- hide_binder_badge: true\n",
    "- comments: true\n",
    "- sticky_rank: 1\n",
    "- author: \"<a href='https://twitter.com/rajkstats'>Raj Kumar</a>\"\n",
    "- description: \"A quick tutorial on how to run spark in Jupyter and Colab Notebooks\"\n",
    "- image: /images/copied_from_nb/spark-on-notebook/blog-head.png\n",
    "- categories: [Jupyter,Spark,Notebooks]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"spark-on-notebook/blog-head.png\" width=\"500\" height=\"200\"/>\n",
    "\n",
    "## Motivation\n",
    "Lately, I had been working on something which required working with spark and put together an analysis reading a large dataset. Usually at work, I would just simply run the code on [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html) / [AWS EMR](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark.html) which comes with pre-defined settings and spark installed and you are ready to run your code in notebooks for doing any kind of these adhoc analysis. \n",
    "\n",
    "In this blog, we would briefly cover how to work interactively with spark in notebook with commonly used languages like **Python**, **R** and **Scala** "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Some Solutions\n",
    "\n",
    "We will go through the following approaches in this blog:\n",
    "\n",
    "- **With Docker**\n",
    "- **Without Docker**\n",
    "- **Running Spark with Google Colab Notebooks**\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Without Docker**\n",
    "\n",
    "Let's get started without using an docker image first:\n",
    "\n",
    "With a little bit of browsing, I was going through this [medium blog by Roshini](https://medium.com/@roshinijohri/spark-with-jupyter-notebook-on-macos-2-0-0-and-higher-c61b971b5007). Though there could be multiple ways to do this. This seems to be the easiest and quickest way to get started. And I am pretty sure that I will be revisting this again and again. So it would be super useful to document the steps on how to get started."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Following are few things you can do to run spark on jupyter notebooks:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Follow the steps **Inspired From Roshni's Blog** (For Mac OS users) to run spark on jupyter notebooks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "brew install apache-spark\n",
    "brew info apache-spark"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Since I've already installed spark on my system, I would just go ahead and print the ouptut of info spark which should look like below if you have sucessfully installed spark\n",
    "\n",
    "![spark info](spark-on-notebook/spark-info.png)\n",
    "\n",
    ">Important: Based on when and which version your system installs spark, change the version in export command below"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "unset SPARK_HOME  (only if you have installed spark earlier)\n",
    "export SPARK_HOME= \"/usr/local/Cellar/apache-spark/3.1.2/libexec/\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also, make sure you have pyspark python package installed:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pip3.9 install pyspark"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![pyspark](spark-on-notebook/pyspark.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run ```pyspark``` or ```spark-shell``` on your terminal to see if spark has configured correctly\n",
    "\n",
    "![spark install](spark-on-notebook/spark-install.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This means that spark is configured, now let's move on to how to interactively run spark with jupyter notebooks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "jupyter notebook"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now copy the following the code to the first cell of your jupyter notebook"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "exec(open(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py')).read())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![spark install](spark-on-notebook/spark-nb.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To access **Spark Application UI**, click on the link available in output of first cell of  jupyter notebook \n",
    "\n",
    "![spark install](spark-on-notebook/spark-app.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **With Docker**\n",
    "\n",
    "Check out this cool project at Github called [Jupyter Docker Stacks](https://github.com/jupyter/docker-stacks). You can setup environments to work with **Python**, **R** and **Scala** with just the following two steps. This would pull the all spark image from dockerhub.\n",
    "\n",
    "Look at the detailed documentation [here](https://jupyter-docker-stacks.readthedocs.io/en/latest/index.html)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the following docker commands to pull the latest image for all spark notebooks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "docker pull jupyter/all-spark-notebook:latest\n",
    "docker run -p 8888:8888 jupyter/all-spark-notebook"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![spark install](spark-on-notebook/docker-spark.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Copy the local host link along with the token at the bottom and hit it on your browser"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![spark update](spark-on-notebook/jupyter-start.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, you should be able to launch Python, R and Scala (**spylon-kernel**) notebooks respectively and initiate spark session within the notebooks and work interactively"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![all spark](spark-on-notebook/all-spark.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **PySpark** "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# Create a Spark Session\n",
    "SparkSession.builder.appName('docker-pyspark').getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![python sc](spark-on-notebook/pyspark-session.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **SparkR**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "library(SparkR)\n",
    "sparkR.session()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![R sc](spark-on-notebook/sparkr-session.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### **Spark Scala**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    "    .builder()\n",
    "    .appName(\"Spark SQL basic example\")\n",
    "    .config(\"spark.some.config.option\",\"some-value\")\n",
    "    .getOrCreate()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![Spark scala](spark-on-notebook/scala-session.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### **Running Spark with Google Colab Notebooks**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You need to just import all the necessary packages needed to run spark in colab."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ">Tip: This blog is built with [fastpages](https://github.com/fastai/fastpages). At the top right corner click on Google Colab badge to run this section in colab without any need to copy & paste the code to google colab."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install wget\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz\n",
    "!tar xf spark-3.1.2-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "import os,wget\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spark = SparkSession.builder.appName('ColabPyspark').getOrCreate()\n",
    "spark"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    ">Tip: Try this section in google colab and share your experience in comments. Thank you for reading !!"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.7 64-bit ('anaconda3': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "interpreter": {
   "hash": "a7a1fc2065abc92b85d42863baff880b7fb862b74cc332143af0e042550830de"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}