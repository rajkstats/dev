{
  
    
        "post0": {
            "title": "Aggregating GH Archive Data with jq",
            "content": ". Motivation . Transforming the raw data into a shape that&#39;s ready for analytics . Goal . In this tutorial, we will be working with GH Archive data which is in JSON format, so before we move forward, we will provide some background on the source and dataset in the next section taken. End goal of this tutorial is to be able to write a bash script which should be able to aggregate the github raw logs using the fields of interest. . What is jq? . jq is a lightweight and flexible command-line JSON processor. | . jq is like sed for JSON data - you can use it to slice and filter and map and transform structured data with the same ease that sed, awk, grep that lets you play with text. . | jq can mangle the data format that you have into the one that you want with very little effort, and the program to do so is often shorter and simpler than you&#39;d expect . | . This command line processor becomes very handy for folks who works with json data. . Installation . jq is written in C and has no runtime dependencies, so it should be possible to build it for nearly any platform. Prebuilt binaries are available for Linux, OS X and Windows. . Download here . Credits:Stephen Dolan . To confirm the installation, type the following in terminal: . !jq --version . jq-1.6 . . Important: Before we move forward, I would like to highlight here that anywhere you see &#8217;!&#8217;,&#8217;%&#8217; in the code has been done only for notebooks, remove it if you&#8217;re not working with notebooks . Download Dataset . Data Source: GH Archive . GH Archive is an open source project to record the public GitHub timeline, archive it, and make it easily accessible for further analysis | GitHub provides 20+ event types, which range from new commits and fork events, to opening new tickets, commenting, and adding members to a project. These events are aggregated into hourly archives | Each archive contains JSON encoded events as reported by the GitHub API. You can download the raw data and apply own processing to it - e.g. write a custom aggregation script, import it into a database, and so on! | . We will download one hour data, simply go to your terminal and follow the following steps . %cd /Users/raj/Desktop/agg-data-with-jq/00_data/ ! wget https://data.gharchive.org/2021-09-01-15.json.gz . /Users/raj/Desktop/agg-data-with-jq/00_data --2021-09-11 16:30:29-- https://data.gharchive.org/2021-09-01-15.json.gz Resolving data.gharchive.org (data.gharchive.org)... 172.67.168.206, 104.21.46.175 Connecting to data.gharchive.org (data.gharchive.org)|172.67.168.206|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 106793608 (102M) [application/gzip] Saving to: ‘2021-09-01-15.json.gz.1’ 2021-09-01-15.json. 100%[===================&gt;] 101.85M 40.8MB/s in 2.5s 2021-09-11 16:30:31 (40.8 MB/s) - ‘2021-09-01-15.json.gz.1’ saved [106793608/106793608] . . Tip: You can edit the yyyy-mm-dd-hh format in wget command, depending upon which timeline you&#8217;re interested in exploring . . Note: This is gzip json compressed file . Built in Operators and Functions . Dot(.) | pipe | | array operator [] | Output a CSV: @csv | Array and Object Construction: [] and {} | Filter select() | Apply a filter or function to an array map() | Unique | length | group_by | . Note: There is lot more in the jq manual for you to look at. . Now again, switch back to your terminal to explore the raw data with jq . Reading a gzipped JSON file (printing only one record) and invoking jq on top of it . 1. Dot(.) . jq . leaves the output unmodified. This is the basic operator and also useful when you&#39;re looking to prettify your json to be able to bring it in the readable format . ! gzip -q -d -c 2021-09-01-15.json.gz | head -n 1 | jq . . { &#34;id&#34;: &#34;17808292672&#34;, &#34;type&#34;: &#34;PushEvent&#34;, &#34;actor&#34;: { &#34;id&#34;: 10745044, &#34;login&#34;: &#34;moises-santillan&#34;, &#34;display_login&#34;: &#34;moises-santillan&#34;, &#34;gravatar_id&#34;: &#34;&#34;, &#34;url&#34;: &#34;https://api.github.com/users/moises-santillan&#34;, &#34;avatar_url&#34;: &#34;https://avatars.githubusercontent.com/u/10745044?&#34; }, &#34;repo&#34;: { &#34;id&#34;: 271635192, &#34;name&#34;: &#34;moises-santillan/covid-19-mexico&#34;, &#34;url&#34;: &#34;https://api.github.com/repos/moises-santillan/covid-19-mexico&#34; }, &#34;payload&#34;: { &#34;push_id&#34;: 7844628061, &#34;size&#34;: 1, &#34;distinct_size&#34;: 1, &#34;ref&#34;: &#34;refs/heads/master&#34;, &#34;head&#34;: &#34;e0dbca5c7f20d224377e6b25cbe1b7d165ce75f4&#34;, &#34;before&#34;: &#34;cf3de81ab4584c0dcdbbee3e42bbeafc2d4b0c94&#34;, &#34;commits&#34;: [ { &#34;sha&#34;: &#34;e0dbca5c7f20d224377e6b25cbe1b7d165ce75f4&#34;, &#34;author&#34;: { &#34;name&#34;: &#34;Moisés Santillán&#34;, &#34;email&#34;: &#34;1b8f84bbb29551654b9a5d0b48cb0e5ec103b514@cinvestav.mx&#34; }, &#34;message&#34;: &#34;$&#34;, &#34;distinct&#34;: true, &#34;url&#34;: &#34;https://api.github.com/repos/moises-santillan/covid-19-mexico/commits/e0dbca5c7f20d224377e6b25cbe1b7d165ce75f4&#34; } ] }, &#34;public&#34;: true, &#34;created_at&#34;: &#34;2021-09-01T15:00:00Z&#34; } . You can use dot(.) operator to access one of the keys in json . ! gzip -q -d -c 2021-09-01-15.json.gz | head -n 1| jq .id . &#34;17808292672&#34; . Also, chaining the key with its nested key allows to access nested object . ! gzip -q -d -c 2021-09-01-15.json.gz | head -n 1| jq .repo.name . &#34;moises-santillan/covid-19-mexico&#34; . 2. pipe operator | : You can use pipe and several operators together in jq for any complex transformations in your data . ! gzip -q -d -c 2021-09-01-15.json.gz | head -n 5| jq .type . &#34;PushEvent&#34; &#34;PullRequestEvent&#34; &#34;DeleteEvent&#34; &#34;WatchEvent&#34; &#34;PushEvent&#34; . 3. array operator [] : This operator is particularly useful in cases when you want to represent a list of github events (JSON) in this case. . Following is the method you can use to create a list of github events in JSON . ! gzip -q -d -c 2021-09-01-15.json.gz | head -n 1 | jq &#39;[.]&#39; . [ { &#34;id&#34;: &#34;17808292672&#34;, &#34;type&#34;: &#34;PushEvent&#34;, &#34;actor&#34;: { &#34;id&#34;: 10745044, &#34;login&#34;: &#34;moises-santillan&#34;, &#34;display_login&#34;: &#34;moises-santillan&#34;, &#34;gravatar_id&#34;: &#34;&#34;, &#34;url&#34;: &#34;https://api.github.com/users/moises-santillan&#34;, &#34;avatar_url&#34;: &#34;https://avatars.githubusercontent.com/u/10745044?&#34; }, &#34;repo&#34;: { &#34;id&#34;: 271635192, &#34;name&#34;: &#34;moises-santillan/covid-19-mexico&#34;, &#34;url&#34;: &#34;https://api.github.com/repos/moises-santillan/covid-19-mexico&#34; }, &#34;payload&#34;: { &#34;push_id&#34;: 7844628061, &#34;size&#34;: 1, &#34;distinct_size&#34;: 1, &#34;ref&#34;: &#34;refs/heads/master&#34;, &#34;head&#34;: &#34;e0dbca5c7f20d224377e6b25cbe1b7d165ce75f4&#34;, &#34;before&#34;: &#34;cf3de81ab4584c0dcdbbee3e42bbeafc2d4b0c94&#34;, &#34;commits&#34;: [ { &#34;sha&#34;: &#34;e0dbca5c7f20d224377e6b25cbe1b7d165ce75f4&#34;, &#34;author&#34;: { &#34;name&#34;: &#34;Moisés Santillán&#34;, &#34;email&#34;: &#34;1b8f84bbb29551654b9a5d0b48cb0e5ec103b514@cinvestav.mx&#34; }, &#34;message&#34;: &#34;$&#34;, &#34;distinct&#34;: true, &#34;url&#34;: &#34;https://api.github.com/repos/moises-santillan/covid-19-mexico/commits/e0dbca5c7f20d224377e6b25cbe1b7d165ce75f4&#34; } ] }, &#34;public&#34;: true, &#34;created_at&#34;: &#34;2021-09-01T15:00:00Z&#34; } ] . Or you can also create arrays by just selecting few keys . ! gzip -q -d -c 2021-09-01-15.json.gz | head -n 2 | jq &#39;[.id, .type, .repo]&#39; . [ &#34;17808292672&#34;, &#34;PushEvent&#34;, { &#34;id&#34;: 271635192, &#34;name&#34;: &#34;moises-santillan/covid-19-mexico&#34;, &#34;url&#34;: &#34;https://api.github.com/repos/moises-santillan/covid-19-mexico&#34; } ] [ &#34;17808292724&#34;, &#34;PullRequestEvent&#34;, { &#34;id&#34;: 279965573, &#34;name&#34;: &#34;konveyor/forklift-ui&#34;, &#34;url&#34;: &#34;https://api.github.com/repos/konveyor/forklift-ui&#34; } ] . In this case, above record have no keys, this becomes pretty handy when you want to map JSON to a csv file . 4. Output a CSV: @csv . Before you output to CSV, make sure all the fields have been flattened, otherwise it would throw an error, like the one we see below . ! gzip -q -d -c 2021-09-01-15.json.gz | head -n 2 | jq &#39;[.id, .type, .repo]| @csv&#39; . jq: error (at &lt;stdin&gt;:1): object ({&#34;id&#34;:27163...) is not valid in a csv row jq: error (at &lt;stdin&gt;:2): object ({&#34;id&#34;:27996...) is not valid in a csv row . ! gzip -q -d -c 2021-09-01-15.json.gz | head -n 3 | jq &#39;[.id, .type, .repo.id,.repo.name,.repo.url]| @csv&#39; . &#34; &#34;17808292672 &#34;, &#34;PushEvent &#34;,271635192, &#34;moises-santillan/covid-19-mexico &#34;, &#34;https://api.github.com/repos/moises-santillan/covid-19-mexico &#34;&#34; &#34; &#34;17808292724 &#34;, &#34;PullRequestEvent &#34;,279965573, &#34;konveyor/forklift-ui &#34;, &#34;https://api.github.com/repos/konveyor/forklift-ui &#34;&#34; &#34; &#34;17808292729 &#34;, &#34;DeleteEvent &#34;,150458335, &#34;openvinotoolkit/model_server &#34;, &#34;https://api.github.com/repos/openvinotoolkit/model_server &#34;&#34; . If you would like to truncate the double quotes in csv file, use the following . ! gzip -q -d -c 2021-09-01-15.json.gz | head -n 3 | jq &#39;[.id, .type, .repo.id,.repo.name,.repo.url]| @csv&#39; | tr -d &#39;&quot; &#39; . 17808292672 , PushEvent ,271635192, moises-santillan/covid-19-mexico , https://api.github.com/repos/moises-santillan/covid-19-mexico 17808292724 , PullRequestEvent ,279965573, konveyor/forklift-ui , https://api.github.com/repos/konveyor/forklift-ui 17808292729 , DeleteEvent ,150458335, openvinotoolkit/model_server , https://api.github.com/repos/openvinotoolkit/model_server . 5. Array and Object Construction: [] and {}** . Let&#39;s say you want to use only few of the objects in json, there&#39;s a way to select few objects and create a new JSON. Following are the two methods: . ! gzip -q -d -c 2021-09-01-15.json.gz | head -n 2 | jq &#39;[.id, .type, .repo]&#39; . [ &#34;17808292672&#34;, &#34;PushEvent&#34;, { &#34;id&#34;: 271635192, &#34;name&#34;: &#34;moises-santillan/covid-19-mexico&#34;, &#34;url&#34;: &#34;https://api.github.com/repos/moises-santillan/covid-19-mexico&#34; } ] [ &#34;17808292724&#34;, &#34;PullRequestEvent&#34;, { &#34;id&#34;: 279965573, &#34;name&#34;: &#34;konveyor/forklift-ui&#34;, &#34;url&#34;: &#34;https://api.github.com/repos/konveyor/forklift-ui&#34; } ] . ! gzip -q -d -c 2021-09-01-15.json.gz | head -n 2 | jq &#39;{id:.id, type: .type, repo: .repo}&#39; . { &#34;id&#34;: &#34;17808292672&#34;, &#34;type&#34;: &#34;PushEvent&#34;, &#34;repo&#34;: { &#34;id&#34;: 271635192, &#34;name&#34;: &#34;moises-santillan/covid-19-mexico&#34;, &#34;url&#34;: &#34;https://api.github.com/repos/moises-santillan/covid-19-mexico&#34; } } { &#34;id&#34;: &#34;17808292724&#34;, &#34;type&#34;: &#34;PullRequestEvent&#34;, &#34;repo&#34;: { &#34;id&#34;: 279965573, &#34;name&#34;: &#34;konveyor/forklift-ui&#34;, &#34;url&#34;: &#34;https://api.github.com/repos/konveyor/forklift-ui&#34; } } . 6. select(): select can be used for querying the JSON . ! gzip -q -d -c 2021-09-01-15.json.gz | head -n 1 | jq &#39;select(.type==&quot;PushEvent&quot; and .actor.login==&quot;moises-santillan&quot;)&#39; . { &#34;id&#34;: &#34;17808292672&#34;, &#34;type&#34;: &#34;PushEvent&#34;, &#34;actor&#34;: { &#34;id&#34;: 10745044, &#34;login&#34;: &#34;moises-santillan&#34;, &#34;display_login&#34;: &#34;moises-santillan&#34;, &#34;gravatar_id&#34;: &#34;&#34;, &#34;url&#34;: &#34;https://api.github.com/users/moises-santillan&#34;, &#34;avatar_url&#34;: &#34;https://avatars.githubusercontent.com/u/10745044?&#34; }, &#34;repo&#34;: { &#34;id&#34;: 271635192, &#34;name&#34;: &#34;moises-santillan/covid-19-mexico&#34;, &#34;url&#34;: &#34;https://api.github.com/repos/moises-santillan/covid-19-mexico&#34; }, &#34;payload&#34;: { &#34;push_id&#34;: 7844628061, &#34;size&#34;: 1, &#34;distinct_size&#34;: 1, &#34;ref&#34;: &#34;refs/heads/master&#34;, &#34;head&#34;: &#34;e0dbca5c7f20d224377e6b25cbe1b7d165ce75f4&#34;, &#34;before&#34;: &#34;cf3de81ab4584c0dcdbbee3e42bbeafc2d4b0c94&#34;, &#34;commits&#34;: [ { &#34;sha&#34;: &#34;e0dbca5c7f20d224377e6b25cbe1b7d165ce75f4&#34;, &#34;author&#34;: { &#34;name&#34;: &#34;Moisés Santillán&#34;, &#34;email&#34;: &#34;1b8f84bbb29551654b9a5d0b48cb0e5ec103b514@cinvestav.mx&#34; }, &#34;message&#34;: &#34;$&#34;, &#34;distinct&#34;: true, &#34;url&#34;: &#34;https://api.github.com/repos/moises-santillan/covid-19-mexico/commits/e0dbca5c7f20d224377e6b25cbe1b7d165ce75f4&#34; } ] }, &#34;public&#34;: true, &#34;created_at&#34;: &#34;2021-09-01T15:00:00Z&#34; } . 7. Apply a filter or function to an array map() : To collapse lists to a single top level element . ! gzip -q -d -c 2021-09-01-15.json.gz | head -n 5| jq -s &#39;map(.type)&#39; . [ &#34;PushEvent&#34;, &#34;PullRequestEvent&#34;, &#34;DeleteEvent&#34;, &#34;WatchEvent&#34;, &#34;PushEvent&#34; ] . 8. unique . ! gzip -q -d -c 2021-09-01-15.json.gz | jq -s &#39;map(.type) | unique&#39; . [ &#34;CommitCommentEvent&#34;, &#34;CreateEvent&#34;, &#34;DeleteEvent&#34;, &#34;ForkEvent&#34;, &#34;GollumEvent&#34;, &#34;IssueCommentEvent&#34;, &#34;IssuesEvent&#34;, &#34;MemberEvent&#34;, &#34;PublicEvent&#34;, &#34;PullRequestEvent&#34;, &#34;PullRequestReviewCommentEvent&#34;, &#34;PullRequestReviewEvent&#34;, &#34;PushEvent&#34;, &#34;ReleaseEvent&#34;, &#34;WatchEvent&#34; ] . 9. length . Here is what length means (taken from docs): . The length of a string is the number of Unicode codepoints it contains (which will be the same as its JSON-encoded length in bytes if it&#39;s pure ASCII). . | The length of an array is the number of elements. . | The length of an object is the number of key-value pairs. . | The length of null is zero. . | . we have flattened the type list to top level elements and so it would return the number of elements, since it&#39;s an array . ! gzip -q -d -c 2021-09-01-15.json.gz | head -n 3 | jq -s &#39;map(.type)&#39; . [ &#34;PushEvent&#34;, &#34;PullRequestEvent&#34;, &#34;DeleteEvent&#34; ] . !gzip -q -d -c 2021-09-01-15.json.gz | head -n 3 | jq -s &#39;map(.type)| length&#39; . 3 . 10. group_by: chunk the results into grouped lists . ! gzip -q -d -c 2021-09-01-15.json.gz | jq -s &#39;group_by (.type)[] | {type: .[0].type, count: length}&#39; . { &#34;type&#34;: &#34;CommitCommentEvent&#34;, &#34;count&#34;: 790 } { &#34;type&#34;: &#34;CreateEvent&#34;, &#34;count&#34;: 24577 } { &#34;type&#34;: &#34;DeleteEvent&#34;, &#34;count&#34;: 7878 } { &#34;type&#34;: &#34;ForkEvent&#34;, &#34;count&#34;: 2803 } { &#34;type&#34;: &#34;GollumEvent&#34;, &#34;count&#34;: 411 } { &#34;type&#34;: &#34;IssueCommentEvent&#34;, &#34;count&#34;: 13276 } { &#34;type&#34;: &#34;IssuesEvent&#34;, &#34;count&#34;: 4377 } { &#34;type&#34;: &#34;MemberEvent&#34;, &#34;count&#34;: 469 } { &#34;type&#34;: &#34;PublicEvent&#34;, &#34;count&#34;: 587 } { &#34;type&#34;: &#34;PullRequestEvent&#34;, &#34;count&#34;: 18561 } { &#34;type&#34;: &#34;PullRequestReviewCommentEvent&#34;, &#34;count&#34;: 3720 } { &#34;type&#34;: &#34;PullRequestReviewEvent&#34;, &#34;count&#34;: 5957 } { &#34;type&#34;: &#34;PushEvent&#34;, &#34;count&#34;: 76918 } { &#34;type&#34;: &#34;ReleaseEvent&#34;, &#34;count&#34;: 1012 } { &#34;type&#34;: &#34;WatchEvent&#34;, &#34;count&#34;: 7398 } . Now that we know enough functions and operators, we can leverage that to write a statement which would aggregate GH archive logs on fields on interest. Let&#39;s see how we can do that. We would also look at how to add and delete fields in the aggregated JSON . Aggregating the GH Archive logs . Adding date_utc and date_hr two arguments in the JSON below and deleting actor_login . ! gzip -q -d -c 2021-09-01-15.json.gz | head -n 3| jq -c --arg date_utc &quot;$(date +&#39;%d-%m-%Y&#39;)&quot; --arg date_hr &quot;$(date)&quot; &#39;. + {date_utc: $date_utc, date_hr : $date_hr}&#39; |jq -s &#39;group_by (.date_utc, .date_hr,.actor.login,.repo.name,.type)[] | {&quot;date_utc&quot; : .[0].date_utc, &quot;date_hr&quot; : .[0].date_hr, &quot;actor_login&quot;:.[0].actor.login,&quot;repo_name&quot;:.[0].repo.name,&quot;type&quot;: .[0].type, count: length}| del(.actor_login)&#39; . { &#34;date_utc&#34;: &#34;13-09-2021&#34;, &#34;date_hr&#34;: &#34;Mon Sep 13 17:20:46 IST 2021&#34;, &#34;actor_login&#34;: &#34;dtrawins&#34;, &#34;repo_name&#34;: &#34;openvinotoolkit/model_server&#34;, &#34;type&#34;: &#34;DeleteEvent&#34;, &#34;count&#34;: 1 } { &#34;date_utc&#34;: &#34;13-09-2021&#34;, &#34;date_hr&#34;: &#34;Mon Sep 13 17:20:46 IST 2021&#34;, &#34;actor_login&#34;: &#34;moises-santillan&#34;, &#34;repo_name&#34;: &#34;moises-santillan/covid-19-mexico&#34;, &#34;type&#34;: &#34;PushEvent&#34;, &#34;count&#34;: 1 } { &#34;date_utc&#34;: &#34;13-09-2021&#34;, &#34;date_hr&#34;: &#34;Mon Sep 13 17:20:46 IST 2021&#34;, &#34;actor_login&#34;: &#34;seanforyou23&#34;, &#34;repo_name&#34;: &#34;konveyor/forklift-ui&#34;, &#34;type&#34;: &#34;PullRequestEvent&#34;, &#34;count&#34;: 1 } . Shell-Script . Now, here is the example to the shell-script which was our end goal which would aggregate data on hourly granularity. Output is being written in csv format and removing double quotes. All the hourly files need to be downloaded in path being provided which would pick the files with .gz extension only. There is a room for lot of improvement in shell-script. A lot more structure can be provided here. Rather, treat this as a reference to get you started with jq. . Snapshot of output of shell-script . . Next Steps . Ingest the hourly aggregates in a database of your choice | Write an Airflow DAG or CRON job to monitor the ETL pipeline | . Inspired From (Further Resources) . jq Docs | Gh Archive | JSON and jq | .",
            "url": "https://rajkstats.dev/jq/bash/2021/09/09/aggregating-data-with-jq.html",
            "relUrl": "/jq/bash/2021/09/09/aggregating-data-with-jq.html",
            "date": " • Sep 9, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Running Spark with Notebooks",
            "content": ". Motivation . Lately, I had been working on something which required working with spark and put together an analysis reading a large dataset. Usually at work, I would just simply run the code on Databricks Notebooks / AWS EMR which comes with pre-defined settings and spark installed and you are ready to run your code in notebooks for doing any kind of these adhoc analysis. . In this blog, we would briefly cover how to work interactively with spark in notebook with commonly used languages like Python, R and Scala . Some Solutions . We will go through the following approaches in this blog: . With Docker | Without Docker | Running Spark with Google Colab Notebooks | . Without Docker . Let&#39;s get started without using an docker image first: . With a little bit of browsing, I was going through this medium blog by Roshini. Though there could be multiple ways to do this. This seems to be the easiest and quickest way to get started. And I am pretty sure that I will be revisting this again and again. So it would be super useful to document the steps on how to get started. . Following are few things you can do to run spark on jupyter notebooks: . Follow the steps Inspired From Roshni&#39;s Blog (For Mac OS users) to run spark on jupyter notebooks . brew install apache-spark brew info apache-spark . Since I&#39;ve already installed spark on my system, I would just go ahead and print the ouptut of info spark which should look like below if you have sucessfully installed spark . . Important: Based on when and which version your system installs spark, change the version in export command below . unset SPARK_HOME (only if you have installed spark earlier) export SPARK_HOME= &quot;/usr/local/Cellar/apache-spark/3.1.2/libexec/&quot; . Also, make sure you have pyspark python package installed: . pip3.9 install pyspark . . Run pyspark or spark-shell on your terminal to see if spark has configured correctly . . This means that spark is configured, now let&#39;s move on to how to interactively run spark with jupyter notebooks . jupyter notebook . Now copy the following the code to the first cell of your jupyter notebook . import os exec(open(os.path.join(os.environ[&quot;SPARK_HOME&quot;], &#39;python/pyspark/shell.py&#39;)).read()) . . To access Spark Application UI, click on the link available in output of first cell of jupyter notebook . . With Docker . Check out this cool project at Github called Jupyter Docker Stacks. You can setup environments to work with Python, R and Scala with just the following two steps. This would pull the all spark image from dockerhub. . Look at the detailed documentation here . Run the following docker commands to pull the latest image for all spark notebooks . docker pull jupyter/all-spark-notebook:latest docker run -p 8888:8888 jupyter/all-spark-notebook . . Copy the local host link along with the token at the bottom and hit it on your browser . . Now, you should be able to launch Python, R and Scala (spylon-kernel) notebooks respectively and initiate spark session within the notebooks and work interactively . . PySpark . from pyspark.sql import SparkSession # Create a Spark Session SparkSession.builder.appName(&#39;docker-pyspark&#39;).getOrCreate() . . SparkR . library(SparkR) sparkR.session() . . Spark Scala . import org.apache.spark.sql.SparkSession val spark = SparkSession .builder() .appName(&quot;Spark SQL basic example&quot;) .config(&quot;spark.some.config.option&quot;,&quot;some-value&quot;) .getOrCreate() . . Running Spark with Google Colab Notebooks . You need to just import all the necessary packages needed to run spark in colab. . . Tip: This blog is built with fastpages. At the top right corner click on Google Colab badge to run this section in colab without any need to copy &amp; paste the code to google colab. . !pip install wget !apt-get install openjdk-8-jdk-headless -qq &gt; /dev/null !wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz !tar xf spark-3.1.2-bin-hadoop2.7.tgz !pip install -q findspark import os,wget os.environ[&quot;JAVA_HOME&quot;] = &quot;/usr/lib/jvm/java-8-openjdk-amd64&quot; os.environ[&quot;SPARK_HOME&quot;] = &quot;/content/spark-3.1.2-bin-hadoop2.7&quot; . import findspark findspark.init() from pyspark.sql import SparkSession . spark = SparkSession.builder.appName(&#39;ColabPyspark&#39;).getOrCreate() spark . . Tip: Try this section in google colab and share your experience in comments. Thank you for reading !! .",
            "url": "https://rajkstats.dev/jupyter/spark/2021/09/02/running-spark-notebooks.html",
            "relUrl": "/jupyter/spark/2021/09/02/running-spark-notebooks.html",
            "date": " • Sep 2, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Re-work of GitDiscoverer.com",
            "content": ". This post was originally posted on LinkedIn . Link to web app - GitDiscoverer .",
            "url": "https://rajkstats.dev/r/shiny/2020/03/26/gitdiscoverer.html",
            "relUrl": "/r/shiny/2020/03/26/gitdiscoverer.html",
            "date": " • Mar 26, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "The Python Package, R users need : rpy2",
            "content": ". Recently, I came across a situation where I would have to write a R library which is not available in Python. I was working on a machine learning pipeline in Python where I had to combine all pre-processing into a one single flow. Ofcourse, you are not limited to these libraries. You can always add your own functions in Python. However, author of that R package has done a pretty good work and in interest of time, I wanted to use it in one of my workflows. . I was in a situation where I had only few options: . Use R and python scripts separately and merge the results at last all together in a different script. | Implement that library in Python | Use rpy2 | rpy2 is a python package which allows you to use R functionalities in Python environment. Basically, you need to import R libraries using rpy2 functions in Python environment. Also, it allows you to convert your R objects to Python objects back and forth (for ex: Converting R and Python dataframes back and forth ). In this article, I will walk you through implementation of stringdist in python which is an awesome package to calculate distance between two strings using different methods. Idea of writing this article here is to show implementation of stringdist R package in python environment using rpy2. . import rpy2 import rpy2.robjects as robjects import rpy2.robjects.packages as rpackages from rpy2.robjects.packages import importr from pandas import read_csv import pandas as pd # pandas2ri to convert dataframes back and forth in R and # python dataframes # useful: when you want to load a R dataframe and then convert # to pandas df or vice-versa from rpy2.robjects import pandas2ri # Installing required packages from R in rpy2 to use function # Importing utils from R to install required packages utils= importr(&#39;utils&#39;) #You can pass a list of R packages in packnames below packnames = (&#39;stringdist&#39;,&#39;base&#39;) # R vector of strings from rpy2.robjects.vectors import StrVector # Selectively install what needs to be install. # We are fancy, just because we can. names_to_install = [x for x in packnames if not rpackages.isinstalled(x)] if len(names_to_install) &gt; 0: utils.install_packages(StrVector(names_to_install)) . Reading a movies dataset with title and ratings and a dataset with corresponding messy movie titles to calculate string distance between two titles using different methods which I have used in this jupyter notebook. . Looking at the datasets: . movies= read_csv(&#39;/Users/raj/Desktop/stringdist/movies.txt&#39;) . . messy_movie_titles= read_csv(&#39;/Users/raj/Desktop/stringdist/ user_queries.txt&#39;) # Assingning column name messy_title messy_movie_titles.columns = [&#39;messy_title&#39;] messy_movie_titles.head() . . #Combining orginal movie title and # messy movie title in one dataframe # Since R stringdist expects an input in same way result=pd.concat([movies[&#39;title&#39;], messy_movie_titles[&#39;messy_title&#39;]], axis=1,ignore_index=False) result.columns = [&#39;title&#39;,&#39;messy_title&#39;] result.head() # Calculating distance between strings # using stringdist R package with methods # Levenshtein ,Cosine And Jaccard Distance # importing stringdist package in python # using importr function stringdist = importr(&#39;stringdist&#39;) #Lets check type of the result object type(result) pandas.core.frame.DataFrame . Need to convert this Python pandas dataframe to R data frame using pandas2ri to pass to stringdist function which expects an R df as input. . pandas2ri.activate() robjects.globalenv[&#39;result&#39;] = result # Calculating Levenshtein distance between the two titles ld =stringdist.stringdist(result[&#39;title&#39;], result[&#39;messy_title&#39;], method=&#39;lv&#39;) result[&#39;Levenshtein_distance&#39;] = 0 result[&#39;Levenshtein_distance&#39;] = ld # Calculating Cosine distance between the two titles cd =stringdist.stringdist(result[&#39;title&#39;], result[&#39;messy_title&#39;], method=&#39;cosine&#39;,q=2) result[&#39;Cosine_distance&#39;] = 0 result[&#39;Cosine_distance&#39;] = cd # Calculating Jaccard distance between the two titles jd =stringdist.stringdist(result[&#39;title&#39;], result[&#39;messy_title&#39;], method=&#39;jaccard&#39;,q=2) result[&#39;Jaccard_distance&#39;] = 0 result[&#39;Jaccard_distance&#39;] = jd #Printing final data frame which contains the results from all distances . . Credits: joyofdata .",
            "url": "https://rajkstats.dev/r/python/2018/04/24/rpy2.html",
            "relUrl": "/r/python/2018/04/24/rpy2.html",
            "date": " • Apr 24, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Hi, I’m Raj, a statistician. I help organisations use data to make well informed business decisions. . In June 2017, I joined the C1X team as a data analyst. . Previously I have: . Self-serve Analytics: Enabled data access to the entire organization by setting up Apache Superset and Metabase. This helped folks access reports in a self-service manner using industry-leading tools. Allowing easy access to rich reports is truly a game-changer in any organization . | Developed a URL classification model to identify and enrich an ad impression to provide more context for campaign selection . | AWS cost optimisation by developing a routing model to match ad impressions with bidders . | Predicting the anomalous behavior in revenue and send an e-mail alert with the affected metrics . | Identifying top monetisation friendly users based on various buying intents. This helped us identify which publisher had a premium audience and made us focus more of our efforts on growing those accounts. . | Built a category reporting system for a large ecommerce client . | Worked on a GDPR use-case for our client for meeting their data removal requirements . | Outside of all this, I love cooking desi food and morning walks. . I’ve lived in Bengaluru, India since 2017 and grew up and completed my education in New Delhi, India . Note: I intend to use this space to write and share stuff on analytics engineering .",
          "url": "https://rajkstats.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  
  

  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rajkstats.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}