{
  
    
        "post0": {
            "title": "Running Spark with Notebooks",
            "content": ". Motivation . Lately, I had been working on something which required working with spark and put together an analysis reading a large dataset. Usually at work, I would just simply run the code on Databricks Notebooks / AWS EMR which comes with pre-defined settings and spark installed and you are ready to run your code in notebooks for doing any kind of these adhoc analysis. . In this blog, we would briefly cover how to work interactively with spark in notebook with commonly used languages like Python, R and Scala . Some Solutions . We will go through the following approaches in this blog: . With Docker | Without Docker | Running Spark with Google Colab Notebooks | . Without Docker . Let&#39;s get started without using an docker image first: . With a little bit of browsing, I was going through this medium blog by Roshini. Though there could be multiple ways to do this. This seems to be the easiest and quickest way to get started. And I am pretty sure that I will be revisting this again and again. So it would be super useful to document the steps on how to get started. . Following are few things you can do to run spark on jupyter notebooks: . Follow the steps Inspired From Roshni&#39;s Blog (For Mac OS users) to run spark on jupyter notebooks . brew install apache-spark brew info apache-spark . Since I&#39;ve already installed spark on my system, I would just go ahead and print the ouptut of info spark which should look like below if you have sucessfully installed spark . . Important: Based on when and which version your system installs spark, change the version in export command below . unset SPARK_HOME (only if you have installed spark earlier) export SPARK_HOME= &quot;/usr/local/Cellar/apache-spark/3.1.2/libexec/&quot; . Also, make sure you have pyspark python package installed: . pip3.9 install pyspark . . Run pyspark or spark-shell on your terminal to see if spark has configured correctly . . This means that spark is configured, now let&#39;s move on to how to interactively run spark with jupyter notebooks . jupyter notebook . Now copy the following the code to the first cell of your jupyter notebook . import os exec(open(os.path.join(os.environ[&quot;SPARK_HOME&quot;], &#39;python/pyspark/shell.py&#39;)).read()) . . To access Spark Application UI, click on the link available in output of first cell of jupyter notebook . . With Docker . Check out this cool project at Github called Jupyter Docker Stacks. You can setup environments to work with Python, R and Scala with just the following two steps. This would pull the all spark image from dockerhub. . Look at the detailed documentation here . Run the following docker commands to pull the latest image for all spark notebooks . docker pull jupyter/all-spark-notebook:latest docker run -p 8888:8888 jupyter/all-spark-notebook . . Copy the local host link along with the token at the bottom and hit it on your browser . . Now, you should be able to launch Python, R and Scala (spylon-kernel) notebooks respectively and initiate spark session within the notebooks and work interactively . . PySpark . . SparkR . . Spark Scala . . Running Spark with Google Colab Notebooks . You need to just import all the necessary packages needed to run spark in colab. . . Tip: This blog is built with fastpages. At the top right corner click on Google Colab badge to run this section in colab without any need to copy &amp; paste the code to google colab. . !pip install wget !apt-get install openjdk-8-jdk-headless -qq &gt; /dev/null !wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz !tar xf spark-3.1.2-bin-hadoop2.7.tgz !pip install -q findspark import os,wget os.environ[&quot;JAVA_HOME&quot;] = &quot;/usr/lib/jvm/java-8-openjdk-amd64&quot; os.environ[&quot;SPARK_HOME&quot;] = &quot;/content/spark-3.1.2-bin-hadoop2.7&quot; . import findspark findspark.init() from pyspark.sql import SparkSession . spark = SparkSession.builder.appName(&#39;ColabPyspark&#39;).getOrCreate() spark . . Tip: Try this section in google colab and share your experience in comments. Thank you for reading !! .",
            "url": "https://rajkstats.dev/jupyter/spark/notebooks/2021/09/02/running-spark-notebooks.html",
            "relUrl": "/jupyter/spark/notebooks/2021/09/02/running-spark-notebooks.html",
            "date": " • Sep 2, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Re-work of GitDiscoverer.com",
            "content": ". This post was originally posted on LinkedIn . Link to web app - GitDiscoverer .",
            "url": "https://rajkstats.dev/r/shiny/2020/03/26/gitdiscoverer.html",
            "relUrl": "/r/shiny/2020/03/26/gitdiscoverer.html",
            "date": " • Mar 26, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "The Python Package, R users need : rpy2",
            "content": ". Recently, I came across a situation where I would have to write a R library which is not available in Python. I was working on a machine learning pipeline in Python where I had to combine all pre-processing into a one single flow. Ofcourse, you are not limited to these libraries. You can always add your own functions in Python. However, author of that R package has done a pretty good work and in interest of time, I wanted to use it in one of my workflows. . I was in a situation where I had only few options: . Use R and python scripts separately and merge the results at last all together in a different script. | Implement that library in Python | Use rpy2 | rpy2 is a python package which allows you to use R functionalities in Python environment. Basically, you need to import R libraries using rpy2 functions in Python environment. Also, it allows you to convert your R objects to Python objects back and forth (for ex: Converting R and Python dataframes back and forth ). In this article, I will walk you through implementation of stringdist in python which is an awesome package to calculate distance between two strings using different methods. Idea of writing this article here is to show implementation of stringdist R package in python environment using rpy2. . import rpy2 import rpy2.robjects as robjects import rpy2.robjects.packages as rpackages from rpy2.robjects.packages import importr from pandas import read_csv import pandas as pd # pandas2ri to convert dataframes back and forth in R and # python dataframes # useful: when you want to load a R dataframe and then convert # to pandas df or vice-versa from rpy2.robjects import pandas2ri # Installing required packages from R in rpy2 to use function # Importing utils from R to install required packages utils= importr(&#39;utils&#39;) #You can pass a list of R packages in packnames below packnames = (&#39;stringdist&#39;,&#39;base&#39;) # R vector of strings from rpy2.robjects.vectors import StrVector # Selectively install what needs to be install. # We are fancy, just because we can. names_to_install = [x for x in packnames if not rpackages.isinstalled(x)] if len(names_to_install) &gt; 0: utils.install_packages(StrVector(names_to_install)) . Reading a movies dataset with title and ratings and a dataset with corresponding messy movie titles to calculate string distance between two titles using different methods which I have used in this jupyter notebook. . Looking at the datasets: . movies= read_csv(&#39;/Users/raj/Desktop/stringdist/movies.txt&#39;) . . messy_movie_titles= read_csv(&#39;/Users/raj/Desktop/stringdist/ user_queries.txt&#39;) # Assingning column name messy_title messy_movie_titles.columns = [&#39;messy_title&#39;] messy_movie_titles.head() . . #Combining orginal movie title and # messy movie title in one dataframe # Since R stringdist expects an input in same way result=pd.concat([movies[&#39;title&#39;], messy_movie_titles[&#39;messy_title&#39;]], axis=1,ignore_index=False) result.columns = [&#39;title&#39;,&#39;messy_title&#39;] result.head() # Calculating distance between strings # using stringdist R package with methods # Levenshtein ,Cosine And Jaccard Distance # importing stringdist package in python # using importr function stringdist = importr(&#39;stringdist&#39;) #Lets check type of the result object type(result) pandas.core.frame.DataFrame . Need to convert this Python pandas dataframe to R data frame using pandas2ri to pass to stringdist function which expects an R df as input. . pandas2ri.activate() robjects.globalenv[&#39;result&#39;] = result # Calculating Levenshtein distance between the two titles ld =stringdist.stringdist(result[&#39;title&#39;], result[&#39;messy_title&#39;], method=&#39;lv&#39;) result[&#39;Levenshtein_distance&#39;] = 0 result[&#39;Levenshtein_distance&#39;] = ld # Calculating Cosine distance between the two titles cd =stringdist.stringdist(result[&#39;title&#39;], result[&#39;messy_title&#39;], method=&#39;cosine&#39;,q=2) result[&#39;Cosine_distance&#39;] = 0 result[&#39;Cosine_distance&#39;] = cd # Calculating Jaccard distance between the two titles jd =stringdist.stringdist(result[&#39;title&#39;], result[&#39;messy_title&#39;], method=&#39;jaccard&#39;,q=2) result[&#39;Jaccard_distance&#39;] = 0 result[&#39;Jaccard_distance&#39;] = jd #Printing final data frame which contains the results from all distances . . Credits: joyofdata .",
            "url": "https://rajkstats.dev/r/python/2018/04/24/rpy2.html",
            "relUrl": "/r/python/2018/04/24/rpy2.html",
            "date": " • Apr 24, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Hi, I’m Raj, a statistician. I help organisations use data to make well informed business decisions. . In June 2017, I joined the C1X team as a data analyst. . Previously I have: . Self-serve Analytics: Enabled data access to the entire organization by setting up Apache Superset and Metabase. This helped folks access reports in a self-service manner using industry-leading tools. Allowing easy access to rich reports is truly a game-changer in any organization . | Developed a URL classification model to identify and enrich an ad impression to provide more context for campaign selection . | AWS cost optimisation by developing a routing model to match ad impressions with bidders . | Predicting the anomalous behavior in revenue and send an e-mail alert with the affected metrics . | Identifying top monetisation friendly users based on various buying intents. This helped us identify which publisher had a premium audience and made us focus more of our efforts on growing those accounts. . | Built a category reporting system for a large ecommerce client . | Worked on a GDPR use-case for our client for meeting their data removal requirements . | Outside of all this, I love cooking desi food and morning walks. . I’ve lived in Bengaluru, India since 2017 and grew up and completed my education in New Delhi, India . Note: I intend to use this space to write and share stuff on analytics engineering .",
          "url": "https://rajkstats.dev/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  
  

  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://rajkstats.dev/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}